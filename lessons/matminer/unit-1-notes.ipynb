{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Materials data science: descriptors and machine learning\n",
    "\n",
    "Welcome to the materials data science lesson. In this session, we will demonstrate how to use `matminer`, `pandas` and `scikit-learn` for machine learning materials properties.\n",
    "\n",
    "The lesson is split into three sections:\n",
    "1. Data retrieval and basic analysis of pandas `DataFrame` objects.\n",
    "2. Generating machine learnable descriptors.\n",
    "3. Training, testing and visualizing machine learning methods with `scikit-learn` and `FigRecipes`.\n",
    "\n",
    "Many more tutorials on how to use matminer (beyond the scope of this workshop) are available in the `matminer_examples` repository, available [here](https://github.com/hackingmaterials/matminer_examples).\n",
    "\n",
    "## Machine learning workflow\n",
    "\n",
    "Firstly, what does a typical machine learning workflow look like? The overall process can be summarized as:\n",
    "1. Take raw inputs, such as a list of compositions, and an associated target property to learn.\n",
    "2. Convert the raw inputs into *descriptors* or *features* that can be learned by machine learning algorithms.\n",
    "3. Train a machine learning model on the data.\n",
    "4. Plot and analyze the performance of the model.\n",
    "\n",
    "<img src=\"resources/ml_workflow.png\" alt=\"machine learning workflow\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "Typically, questions asked by a new practitioner in the field include:\n",
    "- Where do we get the raw data from?\n",
    "- How do we convert the raw data into learnable features?\n",
    "- How can we plot and interpret the results of a model?\n",
    "\n",
    "The `matminer` package has been developed to help make machine learning of materials properties easy and hassle free. The aim of matminer is to connect materials data with data mining algorithms and data visualization.\n",
    "\n",
    "<img src=\"resources/matminer.png\" alt=\"matminer overview\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data retrieval and filtering\n",
    "\n",
    "Matminer interfaces with many materials databases,  including:\n",
    "- Materials Project\n",
    "- Citrine\n",
    "- AFLOW\n",
    "- Materials Data Facility (MDF)\n",
    "- Materials Platform for Data Science (MPDS)\n",
    "\n",
    "In addition, it also includes datasets from published literature. Matminer hosts a repository of 26 (and growing) datasets which comes from published and peer-reviewed machine learning investigations of materials properties or publications of high-throughput computing studies.\n",
    "\n",
    "In this section, we will show how to access and manipulate the datasets from the published literature. More information on accessing other materials databases are detailed in the [matminer_examples](https://github.com/hackingmaterials/matminer_examples) repository.\n",
    "\n",
    "A list of the literature-based datasets can be printed using the `get_available_datasets()` function. This also prints information about what the dataset contains, such as the number of samples, the target properties, and how the data was obtained (e.g., via theory or experiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matminer.datasets import get_available_datasets\n",
    "\n",
    "get_available_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets can be loaded using the `load_dataset()` function and the database name. To save installation space, the datasets are not automatically downloaded when matminer is installed. Instead, the first time the dataset is loaded, it will be downloaded from the internet and stored in the matminer installation directory.\n",
    "\n",
    "Let's load the `dielectric_constant` dataset. It contains 1,056 structures with dielectric properties calculated with DFPT-PBE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matminer.datasets import load_dataset\n",
    "\n",
    "df = load_dataset(\"dielectric_constant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For commonly used datasets, matminer provides convenience loader functions. For example, the dielectric dataset could also be loaded as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matminer.datasets.convenience_loaders import load_dielectric_constant\n",
    "\n",
    "df = load_dielectric_constant()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating and examining pandas `DataFrame` objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are made available as pandas `DataFrame` objects. You can think of these as a type of \"spreadsheet\" object in Python. DataFrames have several useful methods you can use to explore and clean the data, some of which we'll explore below.\n",
    "\n",
    "### Inspecting the dataset\n",
    "\n",
    "The `head()` function prints a summary of the first few rows of a data set. You can scroll across to see more columns. From this, it is easy to see the types of data available in in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, if a dataset is very large, you will be unable to see all the available columns. Instead, you can see the full list of columns using the `columns` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pandas `DataFrame` includes a function called `describe()` that helps determine statistics for the various numerical/categorical columns in the data. Note that the `describe()` function only describes numerical columns by default.\n",
    "\n",
    "Sometimes, the `describe()` function will reveal outliers that indicate mistakes in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access a particular column of `DataFrame` by indexing the object using the column name. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"band_gap\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can access a particular row of a `Dataframe` using the `iloc` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas `DataFrame` objects make it very easy to filter the data based on a specific column. We can use the typical Python comparison operators (==, >, >=, <, etc) to filter numerical values. For example, let's find all entries where the cell volume is greater than 580. We do this by filtering on the `volume` column.\n",
    "\n",
    "Note that we first produce a *boolean mask* â€“ a series of `True` and `False` depending on the comparison. We can then use the mask to filter the `DataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask = df[\"volume\"] >= 580\n",
    "df[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this method of filtering to clean our dataset. For example, if we only wanted our dataset to include semiconductors (materials with a non-zero band gap), we can do this easily by filtering the `band_gap` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semiconductor_df = df[df[\"band_gap\"] > 0]\n",
    "semiconductor_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, a dataset contains many additional columns that are not necessary for machine learning. Before we can train a model on the data, we need to remove any extraneous columns. We can remove whole columns from the dataset using the `drop()` function. This function can be used to drop both rows and columns.\n",
    "\n",
    "The function takes a list of items to drop. For columns, this is column names whereas for rows it is the row number. Finally, the `axis` option specifies whether the data to drop is columns (`1`) or rows (`0`).\n",
    "\n",
    "For example, to remove the `nsites`, `space_group`, `e_electronic`, and `e_total` columns, we can run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = df.drop([\"nsites\", \"space_group\", \"e_electronic\", \"e_total\"],\n",
    "                     axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the cleaned `DataFrame` to see that the columns have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating new columns\n",
    "\n",
    "Pandas `DataFrame` objects also make it easy to perform simple calculations on the data. Think of this as using formulas in Excel spreadsheets. All fundamental Python math operators (such as +, -, /, and \\*) can be used. \n",
    "\n",
    "For example, the dielectric dataset contains the electronic contribution to the dielectric constant ($\\epsilon_\\mathrm{electronic}$, in the `poly_electronic` column) and the total (static) dielectric constant ($\\epsilon_\\mathrm{total}$, in the `poly_total` column). The ionic contribution to the dataset is given by:\n",
    "\n",
    "$$\n",
    "\\epsilon_\\mathrm{ionic} = \\epsilon_\\mathrm{total} - \\epsilon_\\mathrm{electronic}\n",
    "$$\n",
    "\n",
    "Below, we calculate the ionic contribution to the dielectric constant and store it in a new column called `poly_ionic`. This is as simple as assigning the data to the new column, even if the column doesn't already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"poly_ionic\"] = df[\"poly_total\"] - df[\"poly_electronic\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the new data was added correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's practice!\n",
    "\n",
    "Now, let's practice. You'll download a dataset, inspect it, and make sure it is ready to be used for machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
